{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from cvxopt import solvers, matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup\n",
    "state_space = np.tile(np.arange(0, 100, 10), (10, 1)) + np.arange(0, 10, 1).reshape((10, 1))\n",
    "discount_factor = 0.8\n",
    "random_probability = 0.1\n",
    "\n",
    "UP = 0\n",
    "DOWN = 1\n",
    "LEFT = 2\n",
    "RIGHT = 3\n",
    "all_actions = [UP, DOWN, LEFT, RIGHT]\n",
    "\n",
    "\n",
    "def state_to_coordinate(state, state_sets=state_space):\n",
    "    y, x = np.where(state_sets == state)\n",
    "    return (x[0], y[0])\n",
    "\n",
    "def coordinate_to_state(coordinate_x, coordinate_y, state_sets=state_space):\n",
    "    return state_sets[coordinate_y, coordinate_x]\n",
    "\n",
    "def get_valid_actions(state):\n",
    "    valid_actions = all_actions.copy()\n",
    "    x, y = state_to_coordinate(state)\n",
    "    if x == 0:\n",
    "        valid_actions.remove(LEFT)\n",
    "    elif x == 9:\n",
    "        valid_actions.remove(RIGHT)\n",
    "    if y == 0:\n",
    "        valid_actions.remove(UP)\n",
    "    elif y == 9:\n",
    "        valid_actions.remove(DOWN)\n",
    "    return valid_actions\n",
    "\n",
    "def get_next_states_probabilities(cur_state, cur_action):\n",
    "    cur_valid_actions = get_valid_actions(cur_state)\n",
    "    next_probabilities = [0] * 5 # Order UP, DOWN, LEFT, RIGHT, STAY\n",
    "    for action in cur_valid_actions: # w/4 for states inside the grid\n",
    "            next_probabilities[action] = random_probability / 4\n",
    "    \n",
    "    # Calculate the transition probabilities for an action\n",
    "    if (len(cur_valid_actions) == 4): # No boundary cases\n",
    "        next_probabilities[cur_action] += 1 - random_probability\n",
    "        next_probabilities[4] = 0 # Probability to state in the current state is 0\n",
    "    \n",
    "    else: # With probability to move out the grid\n",
    "        next_probabilities[4] = random_probability * (1 - len(cur_valid_actions) / 4)\n",
    "        if cur_action in cur_valid_actions:\n",
    "            next_probabilities[cur_action] += 1 - random_probability\n",
    "        else:\n",
    "            next_probabilities[4] += 1 - random_probability\n",
    "    \n",
    "    return next_probabilities\n",
    "\n",
    "def state_reward(state, reward_function):\n",
    "    x, y = state_to_coordinate(state)\n",
    "    return reward_function[y, x]\n",
    "\n",
    "def get_state_value_function(reward_function):\n",
    "    # Initialize\n",
    "    state_value_function = np.zeros((10, 10))\n",
    "    delta = 1 # Exit flag\n",
    "\n",
    "    while delta > 0.01:\n",
    "        delta = 0 \n",
    "        previous_state_value_function = state_value_function.copy()\n",
    "        # Loop over 100 states\n",
    "        for cur_state in range(100): \n",
    "            cur_x, cur_y = state_to_coordinate(cur_state)\n",
    "            previous_state_value = previous_state_value_function[cur_y, cur_x]\n",
    "            state_value_all_actions = [0] * 4\n",
    "\n",
    "            # Loop over all actions\n",
    "            for cur_action in all_actions:\n",
    "                transition_probabilities = get_next_states_probabilities(cur_state, cur_action) \n",
    "\n",
    "                # Sum over one action\n",
    "                for prob_idx in range(len(transition_probabilities)):\n",
    "                    prob = transition_probabilities[prob_idx]\n",
    "                    if (prob != 0): # Remove invalid case\n",
    "                        # Next state coordinate\n",
    "                        next_x = cur_x\n",
    "                        next_y = cur_y\n",
    "                        if prob_idx == 0: # Move up\n",
    "                            next_y -= 1\n",
    "                        elif prob_idx == 1: # Move down\n",
    "                            next_y += 1\n",
    "                        elif prob_idx == 2: # Move left\n",
    "                            next_x -= 1\n",
    "                        elif prob_idx == 3: # Move right\n",
    "                            next_x += 1\n",
    "                        else: # Stay in the state\n",
    "                            pass\n",
    "                        next_state = coordinate_to_state(next_x, next_y)\n",
    "                        state_value_all_actions[cur_action] += prob * (state_reward(next_state, reward_function) + discount_factor * previous_state_value_function[next_y, next_x])\n",
    "\n",
    "            cur_state_value = max(state_value_all_actions)\n",
    "            state_value_function[cur_y, cur_x] = cur_state_value\n",
    "            delta = max(delta, abs(previous_state_value - cur_state_value)) \n",
    "    return state_value_function\n",
    "\n",
    "def get_optimal_policy_function(state_value_function, reward_function):\n",
    "    # Initialize\n",
    "    optimal_policy_function = np.zeros((10, 10))\n",
    "\n",
    "    # Loop all states\n",
    "    for cur_state in range(100):  \n",
    "        cur_x, cur_y = state_to_coordinate(cur_state)\n",
    "        state_value_all_actions = [0] * 4\n",
    "\n",
    "        # Loop over all actions\n",
    "        for cur_action in all_actions:\n",
    "            transition_probabilities = get_next_states_probabilities(cur_state, cur_action) \n",
    "\n",
    "            # Sum over one action\n",
    "            for prob_idx in range(len(transition_probabilities)):\n",
    "                prob = transition_probabilities[prob_idx]\n",
    "                if (prob != 0): # Remove invalid case\n",
    "                    # Next state coordinate\n",
    "                    next_x = cur_x\n",
    "                    next_y = cur_y\n",
    "                    if prob_idx == 0: # Move up\n",
    "                        next_y -= 1\n",
    "                    elif prob_idx == 1: # Move down\n",
    "                        next_y += 1\n",
    "                    elif prob_idx == 2: # Move left\n",
    "                        next_x -= 1\n",
    "                    elif prob_idx == 3: # Move right\n",
    "                        next_x += 1\n",
    "                    else: # Stay in the state\n",
    "                        pass\n",
    "                    next_state = coordinate_to_state(next_x, next_y)\n",
    "                    state_value_all_actions[cur_action] += prob * (state_reward(next_state, reward_function) + discount_factor * state_value_function[next_y, next_x])\n",
    "        \n",
    "        #state_value_all_actions = np.around(state_value_all_actions, 3) # Round the value to 5 dicimal5\n",
    "        optimal_action = np.argmax(state_value_all_actions)\n",
    "        optimal_policy_function[cur_y, cur_x] = optimal_action\n",
    "        #print('State: ', cur_state)\n",
    "        #print(state_value_all_actions)\n",
    "    return optimal_policy_function.astype(int)\n",
    "\n",
    "def print_optimal_policy_with_arrows(optimal_policy_function):\n",
    "    optimal_policy_symbols = np.chararray((10, 10), unicode=True)\n",
    "    arrow_symbols = ['⇧','⇩','⇦','⇨']\n",
    "#     arrow_symbols = ['↑ ','↓ ','←','→ ']\n",
    "#     arrow_symbols = ['\\21E1','\\u21E3','\\u21E0','\\u21E2']\n",
    "    for i in range(4):\n",
    "        optimal_policy_symbols[optimal_policy_function == i] = arrow_symbols[i]\n",
    "    print(pd.DataFrame(optimal_policy_symbols))\n",
    "\n",
    "def plot_heat_map(heat_array, title):\n",
    "    min_value = np.min(heat_array)\n",
    "    max_value = np.max(heat_array)\n",
    "    plt.pcolor(heat_array, cmap='hot',vmin=min_value, vmax=max_value)\n",
    "    plt.ylim(top=0, bottom=10)\n",
    "    plt.xlim(left=0, right=10)\n",
    "    plt.colorbar()\n",
    "    plt.title(title)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'reward_function_2')"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11e77f240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reward_function_1 = np.zeros((10,10))\n",
    "reward_function_1[-1,-1] = 1\n",
    "\n",
    "\n",
    "reward_function_2 = reward_function_1.copy()\n",
    "locs = [(1,4),(2,4),(3,4),(4,4),(5,4),\n",
    "           (6,4),(1,5),(1,6),(2,6),(3,6),\n",
    "           (3,7),(3,8),(4,8),(5,8),(6,8),\n",
    "           (7,8),(7,6),(7,7),(8,6)]\n",
    "\n",
    "reward_function_2[-1,-1] = 10\n",
    "for loc in locs:\n",
    "    reward_function_2[loc] = -100\n",
    "    \n",
    "fig, ax = plt.subplots(1,2,figsize=(12,6))\n",
    "ax[0].set_ylim(bottom=10, top=0)\n",
    "ax[0].pcolor(reward_function_1)\n",
    "ax[0].set_title('reward_function_1')\n",
    "ax[1].set_ylim(bottom=10, top=0)\n",
    "ax[1].pcolor(reward_function_2)\n",
    "ax[1].set_title('reward_function_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0  1  2  3  4  5  6  7  8  9\n",
      "0  ⇨  ⇨  ⇨  ⇨  ⇨  ⇨  ⇨  ⇨  ⇩  ⇩\n",
      "1  ⇩  ⇩  ⇨  ⇨  ⇨  ⇨  ⇩  ⇩  ⇩  ⇩\n",
      "2  ⇩  ⇩  ⇨  ⇨  ⇨  ⇩  ⇩  ⇩  ⇩  ⇩\n",
      "3  ⇩  ⇩  ⇩  ⇩  ⇩  ⇩  ⇩  ⇩  ⇩  ⇩\n",
      "4  ⇩  ⇩  ⇩  ⇨  ⇨  ⇩  ⇩  ⇩  ⇩  ⇩\n",
      "5  ⇩  ⇩  ⇨  ⇨  ⇨  ⇩  ⇩  ⇩  ⇩  ⇩\n",
      "6  ⇩  ⇨  ⇨  ⇨  ⇨  ⇨  ⇩  ⇩  ⇩  ⇩\n",
      "7  ⇩  ⇨  ⇨  ⇨  ⇨  ⇨  ⇨  ⇩  ⇩  ⇩\n",
      "8  ⇨  ⇨  ⇨  ⇨  ⇨  ⇨  ⇨  ⇨  ⇩  ⇩\n",
      "9  ⇨  ⇨  ⇨  ⇨  ⇨  ⇨  ⇨  ⇨  ⇨  ⇩\n",
      "   0  1  2  3  4  5  6  7  8  9\n",
      "0  ⇩  ⇩  ⇩  ⇦  ⇦  ⇨  ⇨  ⇨  ⇨  ⇩\n",
      "1  ⇩  ⇩  ⇩  ⇦  ⇦  ⇧  ⇨  ⇨  ⇨  ⇩\n",
      "2  ⇩  ⇩  ⇩  ⇦  ⇦  ⇩  ⇨  ⇨  ⇨  ⇩\n",
      "3  ⇩  ⇩  ⇩  ⇦  ⇦  ⇩  ⇩  ⇧  ⇨  ⇩\n",
      "4  ⇩  ⇩  ⇩  ⇦  ⇦  ⇩  ⇩  ⇩  ⇨  ⇩\n",
      "5  ⇩  ⇩  ⇩  ⇦  ⇦  ⇩  ⇩  ⇦  ⇨  ⇩\n",
      "6  ⇩  ⇩  ⇩  ⇩  ⇩  ⇩  ⇦  ⇦  ⇨  ⇩\n",
      "7  ⇩  ⇩  ⇩  ⇩  ⇩  ⇩  ⇦  ⇩  ⇩  ⇩\n",
      "8  ⇨  ⇨  ⇨  ⇩  ⇩  ⇩  ⇩  ⇩  ⇩  ⇩\n",
      "9  ⇨  ⇨  ⇨  ⇨  ⇨  ⇨  ⇨  ⇨  ⇨  ⇩\n"
     ]
    }
   ],
   "source": [
    "optimal_policy_function_1 = np.array([[3, 3, 3, 3, 3, 3, 3, 3, 1, 1],\n",
    "                                   [1, 1, 3, 3, 3, 3, 1, 1, 1, 1],\n",
    "                                   [1, 1, 3, 3, 3, 1, 1, 1, 1, 1],\n",
    "                                   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "                                   [1, 1, 1, 3, 3, 1, 1, 1, 1, 1],\n",
    "                                   [1, 1, 3, 3, 3, 1, 1, 1, 1, 1],\n",
    "                                   [1, 3, 3, 3, 3, 3, 1, 1, 1, 1],\n",
    "                                   [1, 3, 3, 3, 3, 3, 3, 1, 1, 1],\n",
    "                                   [3, 3, 3, 3, 3, 3, 3, 3, 1, 1],\n",
    "                                   [3, 3, 3, 3, 3, 3, 3, 3, 3, 1]])\n",
    "\n",
    "optimal_policy_function_2 = np.array([[1, 1, 1, 2, 2, 3, 3, 3, 3, 1],\n",
    "                                   [1, 1, 1, 2, 2, 0, 3, 3, 3, 1],\n",
    "                                   [1, 1, 1, 2, 2, 1, 3, 3, 3, 1],\n",
    "                                   [1, 1, 1, 2, 2, 1, 1, 0, 3, 1],\n",
    "                                   [1, 1, 1, 2, 2, 1, 1, 1, 3, 1],\n",
    "                                   [1, 1, 1, 2, 2, 1, 1, 2, 3, 1],\n",
    "                                   [1, 1, 1, 1, 1, 1, 2, 2, 3, 1],\n",
    "                                   [1, 1, 1, 1, 1, 1, 2, 1, 1, 1],\n",
    "                                   [3, 3, 3, 1, 1, 1, 1, 1, 1, 1],\n",
    "                                   [3, 3, 3, 3, 3, 3, 3, 3, 3, 1]])\n",
    "\n",
    "# print_optimal_policy_with_arrows(optimal_policy_function_1)\n",
    "# print_optimal_policy_with_arrows(optimal_policy_function_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob(s,policy_act,w=0.1):\n",
    "    \n",
    "    actions = [-10,+10,-1,+1,0]# action = [up,down,left,right,self]\n",
    "    p_temp = np.zeros(5)# action = [up,down,left,right，self]\n",
    "\n",
    "    # preset the random\n",
    "    p_temp[2] = 0 if s%10 == 0 else w/4 # left\n",
    "    p_temp[3] = 0 if (s+1)%10 == 0 else w/4 # right\n",
    "    p_temp[0] = 0 if s-10 <= 0 else w/4 #up\n",
    "    p_temp[1] = 0 if s+10 >= 100 else w/4 #bot\n",
    "    p_temp[-1] = w-sum(p_temp[0:4]) #self\n",
    "    \n",
    "    p = p_temp.copy()\n",
    "    if p_temp[policy_act] != 0:\n",
    "        p[policy_act] += 1-w # move in one direction\n",
    "    else:\n",
    "        p[-1] += 1-w # return to self\n",
    "    transition_row = np.zeros((1,100))\n",
    "    for j,a in enumerate(actions):\n",
    "        if p[j] != 0 :#and s+a in range(100):\n",
    "            transition_row[0][s+a] = p[j]\n",
    "    return transition_row\n",
    "\n",
    "def get_transition_matrix(policy):\n",
    "    transition_matrix = np.zeros((100,100))\n",
    "    policy = policy.reshape((1,100))[0]\n",
    "    for idx,policy_act in enumerate(policy):\n",
    "        transition_matrix[idx] = get_prob(idx,policy_act)\n",
    "    return transition_matrix\n",
    "\n",
    "def get_transition_matrix_under_same_act(a):\n",
    "    transition_matrix = np.zeros((100,100))\n",
    "    \n",
    "    for idx in range(100):\n",
    "        transition_matrix[idx] = get_prob(idx,a)\n",
    "    return transition_matrix\n",
    "\n",
    "# (Pa* -Pa)(I-gamma*Pa*)^-1\n",
    "def T(a,s,policy):\n",
    "    policy = policy.reshape((1,100))\n",
    "    policy_act = policy[0][s]\n",
    "    return np.dot(get_prob(s,policy_act)-get_prob(s,a),\n",
    "                 np.linalg.inv(np.eye(100)-\n",
    "                 0.8*get_transition_matrix_under_same_act(policy_act)))\n",
    "                \n",
    "def get_accuracy(IRL_policy, true_policy):\n",
    "    return np.sum(IRL_policy == true_policy) / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-128-b4d3e09fb83e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0msol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msolvers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mIRL_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msol\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mIRL_state_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_state_value_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIRL_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0mIRL_policy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_optimal_policy_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIRL_state_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIRL_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIRL_policy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_policy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-112-c1b275321f29>\u001b[0m in \u001b[0;36mget_state_value_function\u001b[0;34m(reward_function)\u001b[0m\n\u001b[1;32m     84\u001b[0m                         \u001b[0;32melif\u001b[0m \u001b[0mprob_idx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Move down\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                             \u001b[0mnext_y\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                         \u001b[0;32melif\u001b[0m \u001b[0mprob_idx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Move left\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m                             \u001b[0mnext_x\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                         \u001b[0;32melif\u001b[0m \u001b[0mprob_idx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Move right\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "\n",
    "discount_factor = 0.8\n",
    "cur_reward = reward_function_2\n",
    "Rmax = np.amax(np.abs(cur_reward))\n",
    "cur_policy = optimal_policy_function_2\n",
    "\n",
    "\n",
    "# D \n",
    "T_stack = np.vstack([\n",
    "    -T(a, s, cur_policy)\n",
    "    for s in range(100)\n",
    "    for a in {0,1,2,3} - {cur_policy.reshape(1,-1)[0,s]} # remove the opt action\n",
    "])\n",
    "\n",
    "I_stack1 = np.vstack([\n",
    "    np.eye(1, 100, s)\n",
    "    for s in range(100)\n",
    "    for a in {0,1,2,3} - {cur_policy.reshape(1,-1)[0,s]} # remove the opt action\n",
    "])\n",
    "\n",
    "I_stack2 = np.eye(100)\n",
    "zero_stack1 = np.zeros((300, 100))\n",
    "zero_stack2 = np.zeros((100, 100))\n",
    "\n",
    "\n",
    "D_left = np.vstack([T_stack, T_stack, -I_stack2, I_stack2, -I_stack2,I_stack2])\n",
    "D_middle = np.vstack([I_stack1, zero_stack1, zero_stack2, zero_stack2, zero_stack2,zero_stack2])\n",
    "D_right = np.vstack([zero_stack1, zero_stack1, -I_stack2, -I_stack2, zero_stack2,zero_stack2])\n",
    "D = np.hstack([D_left, D_middle, D_right])\n",
    "\n",
    "# b\n",
    "b = (np.concatenate((np.zeros(800), Rmax * np.ones(200))))\n",
    "\n",
    "# CVXopt\n",
    "G = matrix(D)\n",
    "h = matrix(b)\n",
    "\n",
    "# All accuracies\n",
    "all_acc = list()\n",
    "IRL_best_penalty = 0\n",
    "IRL_best_reward = 0\n",
    "IRL_optimal_state_values = 0\n",
    "IRL_optimal_policy = 0\n",
    "\n",
    "# Solve LP with muptiple penalty factors\n",
    "for penalty_factor in np.arange(0, 5.01, 0.01):\n",
    "    c = matrix(-np.concatenate((np.zeros(100), np.ones(100), np.ones(100) * (-penalty_factor))))\n",
    "    sol = solvers.lp(c, G, h)\n",
    "    IRL_reward = np.array(sol['x'][0:100]).reshape(10, 10).T\n",
    "    IRL_state_values = get_state_value_function(IRL_reward)\n",
    "    IRL_policy = get_optimal_policy_function(IRL_state_values, IRL_reward)\n",
    "    accuracy = get_accuracy(IRL_policy, cur_policy)\n",
    "    # Store the best results\n",
    "    if (len(all_acc) == 0 or accuracy > max(all_acc)):\n",
    "        IRL_best_penalty = penalty_factor\n",
    "        IRL_best_reward = IRL_reward\n",
    "        IRL_optimal_state_values = IRL_state_values\n",
    "        IRL_optimal_policy = IRL_policy\n",
    "    all_acc.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (501,) and (7,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-129-8be8b2ff1611>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# plot the accuracy and penalty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Penalty Factor'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   3259\u001b[0m                       mplDeprecation)\n\u001b[1;32m   3260\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3261\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3262\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3263\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwashold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1715\u001b[0m                     warnings.warn(msg % (label_namer, func.__name__),\n\u001b[1;32m   1716\u001b[0m                                   RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1717\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1718\u001b[0m         \u001b[0mpre_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1719\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpre_doc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1370\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_alias_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1372\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1373\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_grab_next_args\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mseg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mseg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    382\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'plot'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m             raise ValueError(\"x and y must have same first dimension, but \"\n\u001b[0;32m--> 243\u001b[0;31m                              \"have shapes {} and {}\".format(x.shape, y.shape))\n\u001b[0m\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m             raise ValueError(\"x and y can be no greater than 2-D, but have \"\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (501,) and (7,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12305b320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the accuracy and penalty\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.arange(0, 5.01, 0.01), all_acc)\n",
    "plt.xlabel('Penalty Factor')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy VS Penalty Factor')\n",
    "plt.show()\n",
    "\n",
    "print('Penalty factor with best accuracy: ', IRL_best_penalty)\n",
    "print('Corresponding accuracy: ', np.max(all_acc))\n",
    "plot_heat_map(cur_reward, 'Heat map of the ground truth reward')\n",
    "plot_heat_map(IRL_best_reward, 'Heat map of the extracted reward')\n",
    "plot_heat_map(IRL_optimal_state_values, 'Heat map of the optimal state values')\n",
    "print_optimal_policy_with_arrows(IRL_optimal_policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
